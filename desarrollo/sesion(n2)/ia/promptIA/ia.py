# üì¶ Importa m√≥dulos necesarios para configuraci√≥n y conexi√≥n con la API
import os
from dotenv import load_dotenv                      # Para cargar variables de entorno desde un archivo .env
from google.generativeai import configure, GenerativeModel  # SDK oficial para interactuar con Gemini

# ‚úÖ Cargar variables de entorno desde el archivo .env
# Esto permite acceder a la clave de API sin exponerla directamente en el c√≥digo
load_dotenv()

# üîê Configura la API de Gemini usando la clave almacenada en la variable de entorno GEMINI_API_KEY
configure(api_key=os.getenv("GEMINI_API_KEY"))

# ‚úÖ Inicializa el modelo espec√≠fico de Gemini que se va a usar
# "gemini-2.5-flash-lite" es una versi√≥n r√°pida y eficiente, ideal para respuestas √°giles
model = GenerativeModel(model_name="gemini-2.5-flash-lite")

# üß† Funci√≥n principal que env√≠a una pregunta a la IA y devuelve la respuesta
def api_ia(query):
    try:
        # üìù Prompt especializado que define el comportamiento deseado de la IA
        # Le indica que debe resolver problemas algor√≠tmicos en Python, explicar el razonamiento,
        # comentar el c√≥digo y mostrar resultados entendibles en consola
        prompt = """
Resuelve el siguiente problema algor√≠tmico en Python:

Implementa un grafo con costos en las aristas (los costos pueden ser generados aleatoriamente).
Escoge un nodo de inicio y un nodo final, e implementa:

1. El algoritmo de Dijkstra.
2. El algoritmo A* (A-Star).

Tu respuesta debe incluir obligatoriamente:
1. Explicaci√≥n clara del enfoque: c√≥mo se representa el grafo y c√≥mo funcionan Dijkstra y A*.
2. El c√≥digo completo en Python con ambas implementaciones.
3. Comparaci√≥n de los caminos encontrados y sus costos.
4. Ejemplo de ejecuci√≥n en consola mostrando los resultados de ambos algoritmos.
5. Comentarios dentro del c√≥digo explicando los pasos importantes.

No uses JSON, ni envoltorios como "success", "result", "status" o "data".  
La salida debe estar en texto plano y Python comentado.
        """.strip()

        # üöÄ Env√≠a el prompt + la pregunta del usuario al modelo Gemini
        # La IA genera una respuesta basada en el contexto y las instrucciones del prompt
        response = model.generate_content(f"{prompt}\n\nAcci√≥n:\n{query}")

        # üßæ Extrae el texto plano de la respuesta generada por la IA
        response_text = response.text.strip()

        # ‚úÖ Devuelve la respuesta final al llamador (por ejemplo, main.py)
        return response_text

    # ‚ö†Ô∏è Manejo de errores si ocurre alg√∫n problema durante la conexi√≥n o generaci√≥n
    except Exception as e:
        print("‚ùå Error en la API de Gemini:", str(e))
        return {
            "type": "error",
            "data": "Error al procesar la pregunta con la IA.",
            "error": str(e),
        }